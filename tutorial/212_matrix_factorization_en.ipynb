{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coated-southeast",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "\n",
    "Matrix Factorization is a machine learning algorithm used in recommendation engines.\n",
    "\n",
    "A familiar example of a recommendation engine is the \"recommended products\" section of a shopping site. From the past purchase history of user A, we can determine other users B who have similar purchasing tendencies. If user B has purchased a product in the past, there is a high possibility that user A will also want it, so the system encourages purchase by displaying it for A as a recommended product.\n",
    "\n",
    "This time, we will use quantum annealing to perform Matrix Factorization.  \n",
    "\n",
    "References: O’Malley, Daniel, et al. \"Nonnegative/binary matrix factorization with a d-wave quantum annealer.\" PloS one 13.12 (2018): e0206653."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-arabic",
   "metadata": {},
   "source": [
    "We use MovieLens dataset (https://grouplens.org/datasets/movielens/100k/).  \n",
    "It is a dataset of 100,000 ratings (1-5 stars) by 943 users for 1682 movies.\n",
    "\n",
    "First, load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unable-italy",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Data/ml-100k/u.user'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-95f04d149078>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Reading users file:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mu_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'occupation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zip_code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0musers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataDir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'ml-100k/u.user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mu_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#Reading ratings file:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/blueqat/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/blueqat/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/blueqat/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/blueqat/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/blueqat/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/blueqat/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/blueqat/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Data/ml-100k/u.user'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataDir = './Data/'\n",
    "\n",
    "#Reading users file:\n",
    "u_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
    "users = pd.read_csv(dataDir + 'ml-100k/u.user', sep='|', names=u_cols,encoding='latin-1')\n",
    "\n",
    "#Reading ratings file:\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv(dataDir + 'ml-100k/u.data', sep='\\t', names=r_cols,encoding='latin-1')\n",
    "\n",
    "#Reading items file:\n",
    "i_cols = ['movie id', 'movie title' ,'release date','video release date', 'IMDb URL', 'unknown', 'Action', 'Adventure',\n",
    "'Animation', 'Children\\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "items = pd.read_csv(dataDir + 'ml-100k/u.item', sep='|', names=i_cols,\n",
    "encoding='latin-1')\n",
    "\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "\n",
    "ratings_train = pd.read_csv(dataDir + 'ml-100k/ua.base', sep='\\t', names=r_cols, encoding='latin-1')\n",
    "ratings_test = pd.read_csv(dataDir + 'ml-100k/ua.test', sep='\\t', names=r_cols, encoding='latin-1')\n",
    "ratings_train.shape, ratings_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-deposit",
   "metadata": {},
   "source": [
    "The dataset is divided into two parts, one for training and one for test.  \n",
    "The contents are as follows. The rating given by the user indicated by \"user_id\" to the movie indicated by \"movie_id\" is recorded as \"rating\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bright-ambassador",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>unix_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>874965758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>876893171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>878542960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>876893119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>889751712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90565</th>\n",
       "      <td>943</td>\n",
       "      <td>1047</td>\n",
       "      <td>2</td>\n",
       "      <td>875502146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90566</th>\n",
       "      <td>943</td>\n",
       "      <td>1074</td>\n",
       "      <td>4</td>\n",
       "      <td>888640250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90567</th>\n",
       "      <td>943</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>888640250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90568</th>\n",
       "      <td>943</td>\n",
       "      <td>1228</td>\n",
       "      <td>3</td>\n",
       "      <td>888640275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90569</th>\n",
       "      <td>943</td>\n",
       "      <td>1330</td>\n",
       "      <td>3</td>\n",
       "      <td>888692465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90570 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  movie_id  rating  unix_timestamp\n",
       "0            1         1       5       874965758\n",
       "1            1         2       3       876893171\n",
       "2            1         3       4       878542960\n",
       "3            1         4       3       876893119\n",
       "4            1         5       3       889751712\n",
       "...        ...       ...     ...             ...\n",
       "90565      943      1047       2       875502146\n",
       "90566      943      1074       4       888640250\n",
       "90567      943      1188       3       888640250\n",
       "90568      943      1228       3       888640275\n",
       "90569      943      1330       3       888692465\n",
       "\n",
       "[90570 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-illustration",
   "metadata": {},
   "source": [
    "We construct matrix from the data.  \n",
    "The row will be \"user_id\", the column will be \"movei_id\", and each element will be the \"rating\" corresponding to that row and column.\n",
    "\n",
    "Here, from the viewpoint of problem size, only data up to 100 for both \"user_id\" and \"movei_id\" will be extracted, and the matrix will be size of 100 x 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "theoretical-serum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "user_num = 100 # Max.943\n",
    "item_num = 100 # Max.1680\n",
    "\n",
    "def df_to_matrix(df, user_num, item_num, row_name, col_name, values):\n",
    "    df1 = df[df[row_name] <= user_num]\n",
    "    df2 = df1[df1[col_name] <= item_num]\n",
    "    for i in range(1, user_num+1):\n",
    "        if len(df2.loc[(df2.loc[:,row_name]==i)]) == 0:\n",
    "            df2 = df2.append(pd.DataFrame([[i,1]], columns=[row_name, col_name])).fillna(0)\n",
    "    for i in range(1, item_num+1):\n",
    "        if len(df2.loc[(df2.loc[:,col_name]==i)]) == 0:\n",
    "            df2 = df2.append(pd.DataFrame([[1,i]], columns=[row_name, col_name])).fillna(0)\n",
    "    \n",
    "    return np.array(df2.pivot(index = row_name, columns =col_name, values = values).fillna(0))\n",
    "   \n",
    "R = df_to_matrix(ratings_train, user_num, item_num, 'user_id', 'movie_id', 'rating')\n",
    "print(R.shape)\n",
    "\n",
    "R_test = df_to_matrix(ratings_test, user_num, item_num, 'user_id', 'movie_id', 'rating')\n",
    "print(R_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "green-stephen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 3., 4., ..., 4., 3., 5.],\n",
       "       [4., 0., 0., ..., 0., 0., 5.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [4., 0., 3., ..., 5., 0., 5.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-prison",
   "metadata": {},
   "source": [
    "Matrixf Factorization decomposes such a matrix as shown in the figure below.\n",
    "\n",
    "$$\n",
    "V \\approx WH \\\\\n",
    "$$\n",
    "\n",
    "<img src=\"../tutorial-ja/img/212_img.png\" width=\"50%\">\n",
    "\n",
    "\"features\" represents the features that the user and the movie have respectively.  \n",
    "A feature value indicates the preferences and tendencies of a user, for example, \"the user likes action movies\".  The same can be said for movie features, such as \"action movie or not\".  \n",
    "Formulated in this way, we expect that users who like action movies will give high ratings to action movies they have not yet seen.  \n",
    "\n",
    "Matrix Factorization finds $W, H$ that satisfy $V \\approx WH $ with as much accuracy as possible.  \n",
    "In particular, the problem where $W$ is positive and $H$ is restricted to binary (0 or 1) values is called Nonnegative/Binary Matrix Factorization (NBMF).  \n",
    "\n",
    "NBMF repeatedly updates $W, H$ through the following optimization.\n",
    "\n",
    "$$\n",
    "W = \\mathrm{argmin}_{X \\in \\mathbb{R}+^{M\\times K}} \\| V - XH \\|_F  + \\alpha \\| X \\|_F\n",
    "$$\n",
    "\n",
    "$$\n",
    "H = \\mathrm{argmin}_{X \\in \\{0,1\\}^{K\\times N} } \\|V - WX \\|_F\n",
    "$$\n",
    "\n",
    "Furthermore, each column $i$ of $H$ ($H_i$) can be optimized independently as follows\n",
    "\n",
    "$$\n",
    "H_i = \\mathrm{argmin}_{q \\in \\{0,1\\}^k } \\|V_i-Wq \\|_F\n",
    "$$\n",
    "\n",
    "$\\|\\cdot \\|_F$ denotes the Frobenius norm.  \n",
    "Since $H$ was assumed to be binary, we can take advantage of quantum annealing to find $q$.  \n",
    "Specifically, it results in a linear least squares problem, and the following QUBO formulation is known.  \n",
    "\n",
    "$$\n",
    "f(q) = \\sum_i a_i q_i + \\sum_{i<j} b_{ij}q_i q_j  \\\\\n",
    "a_j = \\sum_k W_{kj}(W_{kj} - 2V_{ki}) \\\\\n",
    "b_{jk} = 2\\sum_l W_{lj}W_{lk}\n",
    "$$\n",
    "\n",
    "\n",
    "Reference: Borle, Ajinkya, and Samuel J. Lomonaco. \"Analyzing the quantum annealing approach for solving linear least squares problems.\" International Workshop on Algorithms and Computation. Springer, Cham, 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-newman",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-professional",
   "metadata": {},
   "source": [
    "Let's implement it in blueqat.  \n",
    "The following can be performed with both quantum annealing and QAOA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pointed-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import blueqat.wq as wq\n",
    "from blueqat import vqe\n",
    "from blueqat.pauli import qubo_bit as q\n",
    "\n",
    "class MF():\n",
    "\n",
    "    def __init__(self, V, K, alpha, iterations, Aneal_iteration, method):\n",
    "        self.V = V\n",
    "        self.num_users, self.num_items = V.shape\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.iterations = iterations\n",
    "        self.iteAneal = Aneal_iteration\n",
    "        self.step = 5 # QAOA step\n",
    "        self.method = method\n",
    "\n",
    "    def train(self):\n",
    "        # Initializing user-feature and movie-feature matrix \n",
    "        self.b = np.mean(self.V[np.where(self.V != 0)]) # bias term\n",
    "        self.W = np.abs((np.random.normal(scale=self.b/self.K, size=(self.num_users, self.K))))\n",
    "        self.H = np.abs((np.random.normal(scale=1./2, size=(self.K, self.num_items))))\n",
    "\n",
    "        self.predictedIndex = []\n",
    "        self.itemVecs = []\n",
    "        self.itemVecs_indices = []\n",
    "        for j in range(self.num_items):\n",
    "            self.itemVecs.append([])\n",
    "            self.itemVecs_indices.append([])\n",
    "            for i in range(self.num_users):\n",
    "                if self.V[i, j] > 0:\n",
    "                    self.itemVecs[j].append(self.V[i, j]) # jth column's non 0 elements \n",
    "                    self.itemVecs_indices[j].append(i) # jth column's non 0 indices\n",
    "        \n",
    "        # Stochastic gradient descent for given number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            #np.random.shuffle(self.samples)\n",
    "            self.train_W()\n",
    "            if self.method == 'QAOA':\n",
    "                self.train_H_QAOA()\n",
    "            elif self.method == 'Anneal':\n",
    "                self.train_H_Anneal()\n",
    "            aveError = self.aveError()\n",
    "            if (i+1) % 1 == 0:\n",
    "                print(\"Iteration: %d ; error = %.4f\" % (i+1, aveError))\n",
    "            if i>0 and np.abs(aveError - training_process[-1][1]) < aveError/50: # convergence\n",
    "                break\n",
    "            else:\n",
    "                training_process.append((i, aveError))\n",
    "        return training_process\n",
    "\n",
    "    # Computing mean error\n",
    "    def aveError(self):\n",
    "        xs, ys = self.V.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        l = len(xs)\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += np.abs(self.V[x, y] - predicted[x, y])\n",
    "        return error / l\n",
    "\n",
    "    # Stochastic gradient descent to get optimized W matrix\n",
    "    def train_W(self):\n",
    "        self.Wdiff = np.zeros([self.num_users, self.K])\n",
    "        for i in range(self.num_users):\n",
    "            for q in range(self.K):\n",
    "                for j in range(self.num_items):\n",
    "                    if self.V[i][j] == 0:\n",
    "                        continue\n",
    "                    self.Wdiff[i][q] += 2 * (self.V[i][j] - np.dot(self.W[i,:], self.H[:,j])) * (-1 * self.H[q][j])\n",
    "        self.W = np.abs(self.W - self.alpha * self.Wdiff)\n",
    "\n",
    "    def train_H_QAOA(self):\n",
    "        # use QAOA method\n",
    "        for j in range(self.num_items): #jth column of V    \n",
    "            # define QUBO hamiltonian and solve\n",
    "            self.hamiltonian = 0\n",
    "            for l in range(self.K):\n",
    "                for m in range(l,self.K):\n",
    "                    if l==m: # diagonal elements\n",
    "                        for k in self.itemVecs_indices[j]: #jth columns's k's element\n",
    "                            self.hamiltonian += np.float(self.W[k][l] * (self.W[k][l] - 2 * self.V[k][j])) * q(l)\n",
    "                    else:\n",
    "                        for k in self.itemVecs_indices[j]:\n",
    "                            self.hamiltonian += 2 * np.float(self.W[k][l] * self.W[k][m]) * q(l) * q(m)\n",
    "            result = vqe.Vqe(vqe.QaoaAnsatz(self.hamiltonian, self.step)).run()\n",
    "            self.H[:,j] = list(result.most_common(1)[0][0])\n",
    "            \n",
    "    def train_H_Anneal(self):\n",
    "        # use Q-Anealing\n",
    "        self.JthAneal = 0\n",
    "        for j in range(self.num_items): #jth column of V\n",
    "            for ite in range(self.iteAneal): # iteration of anealing\n",
    "                # define QUBO and solve\n",
    "                self.a = wq.Opt()\n",
    "                self.a.qubo = l_2d_ok = [[0] * self.K for i in range(self.K)] # initialize qubo matrix\n",
    "                for l in range(self.K):\n",
    "                    for m in range(l,self.K):\n",
    "                        if l==m: # diagonal elements\n",
    "                            for k in self.itemVecs_indices[j]: #jth columns's k's element\n",
    "                                self.a.qubo[l][m] += self.W[k][l] * (self.W[k][l] - 2 * self.V[k][j])\n",
    "                        else:\n",
    "                            for k in self.itemVecs_indices[j]:\n",
    "                                self.a.qubo[l][m] += 2 * self.W[k][l] * self.W[k][m]\n",
    "                \n",
    "                tmp = self.a.sa()\n",
    "                tmp_error = self.error_aneal(tmp)\n",
    "                if ite == 0:\n",
    "                    self.anealBit = tmp\n",
    "                    min_error = self.error_aneal(tmp)\n",
    "                else: # update or not\n",
    "                    if tmp_error < min_error:\n",
    "                        self.anealBit = tmp\n",
    "            \n",
    "            self.H[:,j] = self.anealBit\n",
    "                \n",
    "    def error_aneal(self, anealBit):\n",
    "        F = 0\n",
    "        for l in range(self.K):\n",
    "            for m in range(l,self.K):\n",
    "                if l==m: # diagonal elements\n",
    "                    F += self.a.qubo[l][m] * anealBit[l]\n",
    "                else:\n",
    "                    F += self.a.qubo[l][m] * anealBit[l] * anealBit[m]\n",
    "        return abs(F)        \n",
    "\n",
    "    # Full user-movie rating matrix\n",
    "    def full_matrix(self):\n",
    "        return self.W.dot(self.H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-superintendent",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "We will do the learning.  \n",
    "When the reduction in errors converges to a certain degree, learning is terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "expanded-tsunami",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 ; error = 1.1610\n",
      "Iteration: 2 ; error = 0.8512\n",
      "Iteration: 3 ; error = 0.8231\n",
      "Iteration: 4 ; error = 0.7669\n",
      "Iteration: 5 ; error = 0.8519\n",
      "Iteration: 6 ; error = 0.7382\n",
      "Iteration: 7 ; error = 0.7189\n",
      "Iteration: 8 ; error = 0.7329\n",
      "\n",
      "W x H:\n",
      "[[3.07752757 1.57568632 2.76098952 ... 3.07752757 2.21578174 3.29777999]\n",
      " [3.12600217 3.17252369 2.18554561 ... 3.12600217 2.10711165 3.93359492]\n",
      " [1.68016056 2.22742145 0.92386796 ... 1.68016056 2.23645789 2.994783  ]\n",
      " ...\n",
      " [3.03236212 3.19494034 1.54332623 ... 3.03236212 3.34842419 3.8458737 ]\n",
      " [4.35070465 3.87558426 2.96300904 ... 4.35070465 3.45499378 5.10746399]\n",
      " [1.17751116 1.72820597 0.50113742 ... 1.17751116 1.69326523 2.03189509]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed = 42\n",
    "mf = MF(R, K=5, alpha=0.01, iterations=20, Aneal_iteration=1, method='Anneal')\n",
    "training_process = mf.train()\n",
    "print()\n",
    "print(\"W x H:\")\n",
    "print(mf.full_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-client",
   "metadata": {},
   "source": [
    "### Evalation of prediction performance\n",
    "\n",
    "Use the learned features of user and movie to predict user's ratings of movies they haven't seen yet.  \n",
    "The combination of users and movies to be predicted is included in the test data, and the evaluation in the test data is taken as the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "broadband-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = mf.full_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "outdoor-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedRating = []\n",
    "testRating = []\n",
    "score = [[],[],[],[],[]]\n",
    "\n",
    "xs, ys = R_test.nonzero()\n",
    "\n",
    "error_test = 0\n",
    "for x,y in zip(xs, ys):\n",
    "    predictedRating.append(prediction[x][y])\n",
    "    testRating.append(R_test[x][y])\n",
    "    score[int(R_test[x][y] - 1)].append(min(prediction[x][y],5))\n",
    "\n",
    "    error_test += np.abs(R_test[x][y] - prediction[x][y])\n",
    "error_test = error_test / len(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-handling",
   "metadata": {},
   "source": [
    "Let's calculate the average predicted score for each of the test data with ratings of 1, 2, 3, 4, and 5.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "linear-truck",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error = 0.8367807157320516\n",
      "\n",
      "the average predicted score for each of the test data with ratings of 1, 2, 3, 4, and 5.\n",
      "2.473954521232848 \n",
      " 3.3221181082768103 \n",
      " 3.5287032057191428 \n",
      " 3.6094973953846186 \n",
      " 4.093452207661236\n"
     ]
    }
   ],
   "source": [
    "print('Test error =', error_test)\n",
    "print()\n",
    "print('the average predicted score for each of the test data with ratings of 1, 2, 3, 4, and 5.')\n",
    "print(np.mean(score[0]), '\\n',\n",
    "     np.mean(score[1]), '\\n',\n",
    "     np.mean(score[2]), '\\n',\n",
    "     np.mean(score[3]), '\\n',\n",
    "     np.mean(score[4]),)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-suspension",
   "metadata": {},
   "source": [
    "Overall, the test data and the prediction results show the same high/low evaluation trends.  \n",
    "It seems that it is difficult to discriminate points 2, 3, and 4 with this model and learning setting, but it may be possible to discriminate points 1 and 5.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-poster",
   "metadata": {},
   "source": [
    "From the above, we have implemented a recommendation algorithm using Matrix Factorization with Quantum Annealing/QAOA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-watershed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
